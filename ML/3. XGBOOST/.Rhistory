model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 16))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 11, xval = 7, maxdepth = 16))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 11, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
#using only processed data ----
#traning
data = read.csv("train_data.csv")
setwd("B:/SY/SEM_1/DS/courceProject/DS_WaterPotability_Detection/ML/3. decision tree")
#libraries ----
library("rpart.plot")
library("rpart")
library("caret")
#using only processed data ----
#traning
data = read.csv("train_data.csv")
#using only processed data ----
data = read.csv("normalized.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
52/(52+75)
#reading balanced up sampled data ----
data = read.csv("up.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
143/(143+56)
#reading balanced down sampled data ----
data = read.csv("down.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
76/(76+51)
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
#libraries ----
library("rpart.plot")
library("rpart")
library("caret")
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#building model----
model = rpart(Potability ~ ., data = train_data, control = list(cp = 0.000001, minsplit = 10, minbucket = 10, xval = 7, maxdepth = 15))
rpart.plot(model)
#testing model using test----
set.seed(10)
prediction = predict(model, test_data[, -10], type = "class")
cm = confusionMatrix(test_data[, 10], prediction)
print(cm)
97/(97+66)
install.packages("e1071")  # For SVM
install.packages("caret")  # For model evaluation
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
# Display confusion matrix
print(conf_matrix)
source("B:/SY/SEM_1/DS/courceProject/DS_WaterPotability_Detection/ML/4.SVM/SVM.R")
View(data)
data <- data[-1, ]
# Load datasets (adjust the file path)
data <- read.csv("half.csv")
data <- data[, -1]
View(data)
install.packages(xgboost)
install.packages("xgboost")
setwd("B:/SY/SEM_1/DS/courceProject/DS_WaterPotability_Detection/ML/1. decision tree")
setwd("B:/SY/SEM_1/DS/courceProject/DS_WaterPotability_Detection/ML/3. XGBOOST")
#reading balanced down sampled data ----
data = read.csv("down.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
# Libraries ----
library(caret)       # For data partition and evaluation
library(xgboost)     # For XGBoost algorithm
library(Matrix)      # For sparse matrix conversion
#reading balanced down sampled data ----
data = read.csv("down.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#reading balanced down sampled data ----
data = read.csv("down.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#reading balanced up sampled data ----
data = read.csv("up.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
# Prediction and Evaluation on Train Data ----
train_pred <- predict(xgb_model, train_matrix)
train_pred_class <- ifelse(train_pred > 0.5, 1, 0)
cm_train <- confusionMatrix(as.factor(train_pred_class),
as.factor(as.numeric(train_data$Potability) - 1))
print("Confusion Matrix for Train Data:")
print(cm_train)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
source("~/.active-rstudio-document")
# Libraries ----
library(xgboost)
library(Matrix)
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert Data to Matrix Format ----
data_matrix <- xgb.DMatrix(data = as.matrix(data[, -ncol(data)]),
label = as.numeric(data$Potability) - 1)
# XGBoost Parameters ----
params <- list(
objective = "binary:logistic",  # Binary classification with logistic regression
eval_metric = "error",          # Error rate as the evaluation metric
max_depth = 6,                  # Tree depth
eta = 0.3,                      # Learning rate
min_child_weight = 1,           # Minimum instance weight in child nodes
subsample = 0.8,                # Subsample ratio to reduce overfitting
colsample_bytree = 0.8          # Subsample ratio of columns per tree
)
# Perform 5-Fold Cross-Validation ----
set.seed(123)  # Set seed for reproducibility
cv_model <- xgb.cv(params = params,
data = data_matrix,
nrounds = 100,          # Number of boosting rounds
nfold = 5,              # 5-Fold Cross-Validation
showsd = TRUE,          # Show standard deviation of metrics
stratified = TRUE,      # Ensure stratified sampling by class
verbose = 0,            # Suppress intermediate output
early_stopping_rounds = 10)  # Stop if no improvement for 10 rounds
# Display Cross-Validation Results ----
print(cv_model)
cv_model <- xgb.cv(params = params,
data = data_matrix,
nrounds = 100,          # Number of boosting rounds
nfold = 10,              # 10-Fold Cross-Validation
showsd = TRUE,          # Show standard deviation of metrics
stratified = TRUE,      # Ensure stratified sampling by class
verbose = 0,            # Suppress intermediate output
early_stopping_rounds = 10)  # Stop if no improvement for 10 rounds
# Display Cross-Validation Results ----
print(cv_model)
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
# Libraries ----
library(xgboost)
library(Matrix)
library(caret)
# Reading Data ----
data <- read.csv("normalized.csv")  # Load the dataset
data <- data[, -1]  # Remove the first column (if it's an ID)
data$Potability <- as.factor(data$Potability)  # Convert target to factor
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("B:/SY/SEM_1/DS/courceProject/DS_WaterPotability_Detection/ML/3. XGBOOST/XGBOOST.R")
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
# Prediction and Evaluation on Train Data ----
train_pred <- predict(xgb_model, train_matrix)
train_pred_class <- ifelse(train_pred > 0.5, 1, 0)
cm_train <- confusionMatrix(as.factor(train_pred_class),
as.factor(as.numeric(train_data$Potability) - 1))
print("Confusion Matrix for Train Data:")
print(cm_train)
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
#reading balanced up sampled data ----
data = read.csv("up.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
173/173+41
173/173+41
173+41/173
#reading balanced down sampled data ----
data = read.csv("down.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
67/67+51
#reading balanced 50:50 sampled data----
data = read.csv("half.csv")
data = data[, -1]
data$Potability = as.factor(data$Potability)
#split data
set.seed(123)
trainIndex <- createDataPartition(data$Potability, p = 0.9, list = FALSE)
train_data = data[trainIndex, ]
test_data  = data[-trainIndex, ]
# Convert data to matrix format required for XGBoost ----
train_matrix <- xgb.DMatrix(data = as.matrix(train_data[, -ncol(train_data)]),
label = as.numeric(train_data$Potability) - 1)
test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -ncol(test_data)]),
label = as.numeric(test_data$Potability) - 1)
# XGBoost Model Parameters ----
params <- list(
objective = "binary:logistic",  # For binary classification
eval_metric = "error",          # Evaluation metric
max_depth = 6,                  # Maximum depth of a tree
eta = 0.3,                      # Learning rate
nthread = 2,                    # Number of CPU threads
min_child_weight = 1            # Minimum sum of instance weight in a child
)
# Train the XGBoost Model ----
set.seed(123)
xgb_model <- xgb.train(params = params,
data = train_matrix,
nrounds = 100,    # Number of boosting rounds
watchlist = list(train = train_matrix),
verbose = 0)
# Prediction and Evaluation on Test Data ----
test_pred <- predict(xgb_model, test_matrix)
test_pred_class <- ifelse(test_pred > 0.5, 1, 0)
cm_test <- confusionMatrix(as.factor(test_pred_class),
as.factor(as.numeric(test_data$Potability) - 1))
print("Confusion Matrix for Test Data:")
print(cm_test)
